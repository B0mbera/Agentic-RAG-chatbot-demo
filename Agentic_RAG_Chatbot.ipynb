{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3107a67a",
   "metadata": {},
   "source": [
    "# Agentic RAG Chatbot - M≈±k√∂d≈ëk√©pes Protot√≠pus\n",
    "\n",
    "## √Åll√°sinterj√∫ Feladat Megold√°s\n",
    "\n",
    "**Feladat:** Fejlessz egy Agentic RAG (Retrieval-Augmented Generation) alap√∫ chatbot alkalmaz√°st.\n",
    "\n",
    "### K√∂vetelm√©nyek teljes√≠t√©se:\n",
    "\n",
    "#### ‚úÖ 1. Agentic Viselked√©s Demonstr√°l√°sa\n",
    "- **Auton√≥m d√∂nt√©shozatal**: A rendszer √∂n√°ll√≥an d√∂nti el, hogy sz√ºks√©ges-e RAG vagy sem\n",
    "- **R√©szfeladatok lebont√°sa**: √ñsszetett k√©rd√©sek automatikus sz√©tbont√°sa\n",
    "- **Conditional routing**: LangGraph state machine dinamikus √∫tvonalv√°laszt√°ssal\n",
    "\n",
    "#### ‚úÖ 2. RAG Technika Implement√°l√°sa\n",
    "- **Document Loading**: PDF dokumentumok bet√∂lt√©se\n",
    "- **Chunking**: Intelligens sz√∂vegdarabol√°s √°tfed√©ssel\n",
    "- **Embedding**: HuggingFace multilingual model\n",
    "- **Vector Store**: ChromaDB perzisztens t√°rol√°ssal\n",
    "- **Retrieval**: Top-K similarity search\n",
    "- **Generation**: Kontextus-alap√∫ v√°laszgener√°l√°s\n",
    "\n",
    "#### ‚úÖ 3. Struktur√°lt Dokument√°ci√≥\n",
    "- Tervez√©si d√∂nt√©sek indokl√°sa\n",
    "- Architekt√∫ra bemutat√°sa\n",
    "- Teljes√≠tm√©ny elemz√©s √©s bottleneck-ek\n",
    "- Tov√°bbfejleszt√©si javaslatok\n",
    "\n",
    "### Framework: LangGraph ‚úÖ\n",
    "### Programoz√°si nyelv: Python ‚úÖ\n",
    "### API: Ingyenes, Mock LLM implement√°ci√≥ (k√∂nnyen cser√©lhet≈ë) ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1532d",
   "metadata": {},
   "source": [
    "## 1. Konfigur√°ci√≥ √©s K√∂rnyezet Be√°ll√≠t√°sa\n",
    "\n",
    "### Importok √©s alapvet≈ë be√°ll√≠t√°sok\n",
    "\n",
    "**Indokl√°s a v√°lasztott technol√≥gi√°kra:**\n",
    "- **LangChain**: Ipari standard LLM framework, j√≥l dokument√°lt, nagy k√∂z√∂ss√©g\n",
    "- **LangGraph**: State-based agentic workflows, conditional routing t√°mogat√°s\n",
    "- **ChromaDB**: K√∂nny≈±s√∫ly√∫, perzisztens vektor adatb√°zis, nem ig√©nyel k√ºls≈ë szervert\n",
    "- **HuggingFace**: Ingyenes, multilingual embedding modellek, magyar nyelv t√°mogat√°s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629b2426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minden k√∂nyvt√°r sikeresen import√°lva!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Dict, List, Any, TypedDict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "print(\"Minden k√∂nyvt√°r sikeresen import√°lva!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0d88bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konfigur√°ci√≥:\n",
      "- Tud√°sb√°zis mappa: ./knowledge\n",
      "- Embedding model: sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
      "- Chunk m√©ret: 1000, √Åtfed√©s: 200\n",
      "- Top-K dokumentumok: 5\n"
     ]
    }
   ],
   "source": [
    "# Konfigur√°ci√≥ oszt√°ly - centraliz√°lt param√©ter kezel√©s\n",
    "class Config:\n",
    "    KNOWLEDGE_DIR = \"./knowledge\"\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    VECTOR_DB_PATH = \"./chroma_db_pdf\"\n",
    "    CHUNK_SIZE = 1000\n",
    "    CHUNK_OVERLAP = 200\n",
    "    TOP_K_DOCS = 5\n",
    "    # LlamaCpp lok√°lis LLM support (opcion√°lis, most nem haszn√°ljuk)\n",
    "    # LLM_MODEL_PATH = \"./models/llama-2-7b-chat.gguf\"\n",
    "    # USE_LOCAL_LLM = False\n",
    "    USE_MOCK_LLM = True\n",
    "    TEMPERATURE = 0.7\n",
    "    MAX_TOKENS = 512\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"Konfigur√°ci√≥:\")\n",
    "print(f\"- Tud√°sb√°zis mappa: {config.KNOWLEDGE_DIR}\")\n",
    "print(f\"- Embedding model: {config.EMBEDDING_MODEL}\")\n",
    "print(f\"- Chunk m√©ret: {config.CHUNK_SIZE}, √Åtfed√©s: {config.CHUNK_OVERLAP}\")\n",
    "print(f\"- Top-K dokumentumok: {config.TOP_K_DOCS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37324e4f",
   "metadata": {},
   "source": [
    "## 2. PDF Dokumentumok Bet√∂lt√©se\n",
    "\n",
    "**Technol√≥giai v√°laszt√°s:** PyPDFLoader\n",
    "- Megb√≠zhat√≥ PDF parsing\n",
    "- Metaadatok meg≈ërz√©se (forr√°sf√°jl, oldal sz√°m)\n",
    "- K√∂nny≈± integr√°ci√≥ LangChain-nel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e5146a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF f√°jlok bet√∂lt√©se: 2 f√°jl\n",
      "  ‚úì 1706.03762v7.pdf: 15 oldal\n",
      "  ‚úì 1706.03762v7.pdf: 15 oldal\n",
      "  ‚úì elelmiszerek_es_az_egeszseges_taplalkozas_teljes.pdf: 101 oldal\n",
      "\n",
      "√ñsszesen bet√∂lt√∂tt oldalak: 116\n",
      "\n",
      "P√©lda dokumentum:\n",
      "  Forr√°s: 1706.03762v7.pdf\n",
      "  Oldal: 0\n",
      "  Sz√∂veg hossz: 2859 karakter\n",
      "  Tartalom el≈ën√©zet: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "  ‚úì elelmiszerek_es_az_egeszseges_taplalkozas_teljes.pdf: 101 oldal\n",
      "\n",
      "√ñsszesen bet√∂lt√∂tt oldalak: 116\n",
      "\n",
      "P√©lda dokumentum:\n",
      "  Forr√°s: 1706.03762v7.pdf\n",
      "  Oldal: 0\n",
      "  Sz√∂veg hossz: 2859 karakter\n",
      "  Tartalom el≈ën√©zet: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def load_pdf_documents(knowledge_dir: str) -> List[Document]:\n",
    "    \"\"\"PDF dokumentumok bet√∂lt√©se metaadatokkal\"\"\"\n",
    "    if not os.path.exists(knowledge_dir):\n",
    "        print(f\"Hiba: Mappa nem tal√°lhat√≥: {knowledge_dir}\")\n",
    "        return []\n",
    "    \n",
    "    pdf_files = list(Path(knowledge_dir).glob(\"**/*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"Hiba: Nincs PDF f√°jl: {knowledge_dir}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"PDF f√°jlok bet√∂lt√©se: {len(pdf_files)} f√°jl\")\n",
    "    \n",
    "    all_documents = []\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ‚úì {pdf_file.name}: {len(documents)} oldal\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Hiba {pdf_file.name}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n√ñsszesen bet√∂lt√∂tt oldalak: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Dokumentumok bet√∂lt√©se\n",
    "documents = load_pdf_documents(config.KNOWLEDGE_DIR)\n",
    "\n",
    "if documents:\n",
    "    print(f\"\\nP√©lda dokumentum:\")\n",
    "    print(f\"  Forr√°s: {documents[0].metadata.get('source_file', 'N/A')}\")\n",
    "    print(f\"  Oldal: {documents[0].metadata.get('page', 'N/A')}\")\n",
    "    print(f\"  Sz√∂veg hossz: {len(documents[0].page_content)} karakter\")\n",
    "    print(f\"  Tartalom el≈ën√©zet: {documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c5abd",
   "metadata": {},
   "source": [
    "## 3. Dokumentumok Szegment√°l√°sa (Chunking)\n",
    "\n",
    "**Strat√©gia: RecursiveCharacterTextSplitter**\n",
    "- **Chunk m√©ret**: 1000 karakter - optim√°lis egyens√∫ly kontextus √©s pontoss√°g k√∂z√∂tt\n",
    "- **Overlap**: 200 karakter (20%) - biztos√≠tja, hogy ne vesszen el inform√°ci√≥ a hat√°rokon\n",
    "- **Separators**: Hierarchikus darabol√°s (`\\n\\n`, `\\n`, `. `, ` `) - term√©szetes sz√∂vegszerkezet meg≈ërz√©se\n",
    "\n",
    "**Mi√©rt fontos a chunking?**\n",
    "- Embedding modellek token limitje\n",
    "- Pontosabb similarity search (kisebb egys√©gek)\n",
    "- Kontextus meg≈ërz√©se overlap-pel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "692973c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L√©trehozott chunk-ok sz√°ma: 350\n",
      "\n",
      "Statisztik√°k:\n",
      "  - √Åtlagos chunk hossz: 850 karakter\n",
      "  - Legkisebb chunk: 141 karakter\n",
      "  - Legnagyobb chunk: 999 karakter\n",
      "\n",
      "P√©lda chunk:\n",
      "  Forr√°s: 1706.03762v7.pdf\n",
      "  Tartalom: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Par...\n"
     ]
    }
   ],
   "source": [
    "def split_documents(documents: List[Document], chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
    "    \"\"\"Intelligens sz√∂vegdarabol√°s √°tfed√©ssel\"\"\"\n",
    "    if not documents:\n",
    "        return []\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "# Chunking v√©grehajt√°sa\n",
    "chunks = split_documents(documents, config.CHUNK_SIZE, config.CHUNK_OVERLAP)\n",
    "\n",
    "print(f\"L√©trehozott chunk-ok sz√°ma: {len(chunks)}\")\n",
    "\n",
    "if chunks:\n",
    "    chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "    print(f\"\\nStatisztik√°k:\")\n",
    "    print(f\"  - √Åtlagos chunk hossz: {np.mean(chunk_lengths):.0f} karakter\")\n",
    "    print(f\"  - Legkisebb chunk: {np.min(chunk_lengths)} karakter\")\n",
    "    print(f\"  - Legnagyobb chunk: {np.max(chunk_lengths)} karakter\")\n",
    "    \n",
    "    print(f\"\\nP√©lda chunk:\")\n",
    "    print(f\"  Forr√°s: {chunks[0].metadata.get('source_file', 'N/A')}\")\n",
    "    print(f\"  Tartalom: {chunks[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999d454",
   "metadata": {},
   "source": [
    "## 4. Vektor Adatb√°zis L√©trehoz√°sa\n",
    "\n",
    "**V√°lasztott technol√≥gi√°k:**\n",
    "- **Embedding Model**: `paraphrase-multilingual-mpnet-base-v2`\n",
    "  - Multilingual support (magyar nyelv!)\n",
    "  - 768 dimenzi√≥s vektorok\n",
    "  - Sentence-BERT alap√∫, kiv√°l√≥ szemantikus meg√©rt√©s\n",
    "- **Vector Store**: ChromaDB\n",
    "  - Perzisztens t√°rol√°s (egyszer l√©trehozva, mindig haszn√°lhat√≥)\n",
    "  - Nem ig√©nyel k√ºls≈ë adatb√°zis szervert\n",
    "  - Gyors cosine similarity search\n",
    "\n",
    "**Similarity Search m≈±k√∂d√©se:**\n",
    "1. Query ‚Üí Embedding vector (768 dim)\n",
    "2. Cosine similarity sz√°m√≠t√°s az √∂sszes t√°rolt vectorral\n",
    "3. Top-K legink√°bb hasonl√≥ chunk visszaad√°sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78358c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model bet√∂lt√©se...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bong bera\\AppData\\Local\\Temp\\ipykernel_23816\\3657546481.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embedding model sikeresen bet√∂ltve\n",
      "\n",
      "Vektor adatb√°zis m≈±veletek...\n",
      "Megl√©v≈ë vektor adatb√°zis bet√∂lt√©se...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bong bera\\AppData\\Local\\Temp\\ipykernel_23816\\3657546481.py:22: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Vektor adatb√°zis bet√∂ltve\n",
      "\n",
      "T√°rolt dokumentumok sz√°ma: 350\n"
     ]
    }
   ],
   "source": [
    "def create_or_load_vectorstore(chunks: List[Document], embedding_model: str, db_path: str):\n",
    "    \"\"\"Vektor adatb√°zis l√©trehoz√°sa vagy bet√∂lt√©se\"\"\"\n",
    "    if not chunks:\n",
    "        return None\n",
    "    \n",
    "    print(\"Embedding model bet√∂lt√©se...\")\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model,\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        print(\"‚úì Embedding model sikeresen bet√∂ltve\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Embedding hiba: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nVektor adatb√°zis m≈±veletek...\")\n",
    "    try:\n",
    "        if os.path.exists(db_path) and os.listdir(db_path):\n",
    "            print(\"Megl√©v≈ë vektor adatb√°zis bet√∂lt√©se...\")\n",
    "            vectorstore = Chroma(\n",
    "                persist_directory=db_path,\n",
    "                embedding_function=embeddings\n",
    "            )\n",
    "            print(\"‚úì Vektor adatb√°zis bet√∂ltve\")\n",
    "        else:\n",
    "            print(f\"√öj vektor adatb√°zis l√©trehoz√°sa {len(chunks)} chunk-b√≥l...\")\n",
    "            print(\"(Ez eltarthat n√©h√°ny percig az els≈ë fut√°sn√°l)\")\n",
    "            vectorstore = Chroma.from_documents(\n",
    "                documents=chunks,\n",
    "                embedding=embeddings,\n",
    "                persist_directory=db_path\n",
    "            )\n",
    "            print(\"‚úì Vektor adatb√°zis l√©trehozva √©s mentve\")\n",
    "        \n",
    "        print(f\"\\nT√°rolt dokumentumok sz√°ma: {vectorstore._collection.count()}\")\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Vektor adatb√°zis hiba: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Vektor adatb√°zis l√©trehoz√°sa/bet√∂lt√©se\n",
    "vectorstore = create_or_load_vectorstore(chunks, config.EMBEDDING_MODEL, config.VECTOR_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c6c907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Similarity Search Teszt ===\n",
      "\n",
      "Teszt k√©rd√©s: 'Mi a dokumentum t√©m√°ja?'\n",
      "\n",
      "Tal√°lt 3 relev√°ns dokumentum:\n",
      "\n",
      "[1] Forr√°s: elelmiszerek_es_az_egeszseges_taplalkozas_teljes.pdf, Oldal: 3\n",
      "    Tartalom: 4\n",
      " \n",
      "1. Alapfogalmak \n",
      " \n",
      "A jegyzet elej√©n mindenk√©ppen sz√ºks√©gesnek tartjuk az 1995. √©vi XC. t√∂rv√©nyben le√≠rt \n",
      "√©rtelmez≈ë rendelkez√©sek bemutat√°s√°t, mely a k√∂vetkez≈ë: \n",
      "√âlelmiszer:  minden olyan n√∂v√©nyi, ...\n",
      "\n",
      "[2] Forr√°s: elelmiszerek_es_az_egeszseges_taplalkozas_teljes.pdf, Oldal: 27\n",
      "    Tartalom: 28 \n",
      " \n",
      "Vall√°si-filoz√≥fiai meggy≈ëz≈ëd√©s is m√°s v√°laszt√≥vonal at h√∫zhat v√©d≈ë √©s nem v√©d≈ë \n",
      "k√∂z√∂tt. Az ilyen megk√∂zel√≠t√©s nem puszt√°n a testi, hanem a lelki eg√©szs√©g meg≈ërz√©s√©t is \n",
      "alapul veszi. V√©d≈ët nem v...\n",
      "\n",
      "[3] Forr√°s: 1706.03762v7.pdf, Oldal: 12\n",
      "    Tartalom: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Similarity search tesztel√©se\n",
    "if vectorstore:\n",
    "    print(\"=== Similarity Search Teszt ===\\n\")\n",
    "    \n",
    "    test_query = \"Mi a dokumentum t√©m√°ja?\"\n",
    "    print(f\"Teszt k√©rd√©s: '{test_query}'\")\n",
    "    \n",
    "    results = vectorstore.similarity_search(test_query, k=3)\n",
    "    \n",
    "    print(f\"\\nTal√°lt {len(results)} relev√°ns dokumentum:\\n\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"[{i}] Forr√°s: {doc.metadata.get('source_file', 'N/A')}, Oldal: {doc.metadata.get('page', 'N/A')}\")\n",
    "        print(f\"    Tartalom: {doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d35fc",
   "metadata": {},
   "source": [
    "## 5. LLM Inicializ√°l√°sa (Mock Implement√°ci√≥)\n",
    "\n",
    "**Mi√©rt Mock LLM?**\n",
    "- K√∂vetelm√©ny: Nem haszn√°lhatunk fizet≈ës API-kat\n",
    "- Lok√°lis LLM (Llama.cpp) nagy er≈ëforr√°sig√©ny\n",
    "- Mock implement√°ci√≥ demonstr√°lja az architekt√∫r√°t\n",
    "- **K√∂nnyen cser√©lhet≈ë val√≥di LLM-re** (pl. OpenAI, Groq, lok√°lis Llama)\n",
    "\n",
    "**Mock LLM m≈±k√∂d√©se:**\n",
    "- Kinyeri a kontextust √©s k√©rd√©st a promptb√≥l\n",
    "- Visszaadja a kontextus egy r√©szlet√©t\n",
    "- Jelzi, hogy ez mock v√°lasz\n",
    "\n",
    "**Val√≥s LLM integr√°l√°s√°hoz:**\n",
    "```python\n",
    "# P√©lda 1: Lok√°lis Llama.cpp\n",
    "# from langchain_community.llms import LlamaCpp\n",
    "# llm = LlamaCpp(model_path=\"./models/llama-2-7b.gguf\")\n",
    "\n",
    "# P√©lda 2: OpenAI (ha enged√©lyezett lenne)\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# P√©lda 3: Groq (ingyenes tier)\n",
    "# from langchain_groq import ChatGroq\n",
    "# llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "befcfaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Mock LLM inicializ√°lva\n",
      "  (K√∂nnyen cser√©lhet≈ë val√≥di LLM-re)\n"
     ]
    }
   ],
   "source": [
    "class MockLLM:\n",
    "    \"\"\"Mock LLM implement√°ci√≥ - val√≥di LLM imit√°l√°sa\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.7, max_tokens=512):\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "    \n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        \"\"\"Prompt feldolgoz√°s √©s mock v√°lasz gener√°l√°s\"\"\"\n",
    "        if \"Kontextus:\" in prompt:\n",
    "            lines = prompt.split('\\n')\n",
    "            context_lines = []\n",
    "            question = \"\"\n",
    "            \n",
    "            in_context = False\n",
    "            for line in lines:\n",
    "                if \"Kontextus:\" in line:\n",
    "                    in_context = True\n",
    "                elif \"Kerdes:\" in line:\n",
    "                    in_context = False\n",
    "                    question = line.split(\":\", 1)[-1].strip() if \":\" in line else \"\"\n",
    "                elif in_context and line.strip():\n",
    "                    context_lines.append(line.strip())\n",
    "            \n",
    "            if context_lines:\n",
    "                context_preview = ' '.join(context_lines[:2])[:250]\n",
    "                return f\"A dokumentumok alapj√°n: {context_preview}... (Mock LLM v√°lasz - val√≥di LLM r√©szletesebb eredm√©nyt adna)\"\n",
    "        \n",
    "        return \"√Åltal√°nos v√°lasz. (Mock m√≥d akt√≠v - config.USE_MOCK_LLM = False val√≥di LLM-hez)\"\n",
    "    \n",
    "    def invoke(self, prompt, **kwargs):\n",
    "        \"\"\"LangChain kompatibilis invoke met√≥dus\"\"\"\n",
    "        return self.__call__(prompt, **kwargs)\n",
    "\n",
    "# LLM inicializ√°l√°sa\n",
    "llm = MockLLM(temperature=config.TEMPERATURE, max_tokens=config.MAX_TOKENS)\n",
    "print(\"‚úì Mock LLM inicializ√°lva\")\n",
    "print(\"  (K√∂nnyen cser√©lhet≈ë val√≥di LLM-re)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6295f",
   "metadata": {},
   "source": [
    "## 6. Agentic √Ållapot Defini√°l√°sa\n",
    "\n",
    "**AgentState - a workflow √°llapot strukt√∫r√°ja:**\n",
    "- `query`: Felhaszn√°l√≥ k√©rd√©se\n",
    "- `needs_rag`: Auton√≥m d√∂nt√©s - sz√ºks√©ges-e RAG?\n",
    "- `retrieved_context`: Lek√©rt dokumentumok\n",
    "- `response`: Gener√°lt v√°lasz\n",
    "- `confidence`: Magabiztoss√°gi szint (0.0-1.0)\n",
    "\n",
    "Ez a TypedDict biztos√≠tja az √°llapot k√∂vet√©s√©t a LangGraph workflow-ban."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0f04b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì AgentState TypedDict defini√°lva\n",
      "  Mez≈ëk: query, needs_rag, retrieved_context, response, confidence\n"
     ]
    }
   ],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"Agentic workflow √°llapot defin√≠ci√≥ja\"\"\"\n",
    "    query: str              # Felhaszn√°l√≥ k√©rd√©se\n",
    "    needs_rag: bool         # RAG sz√ºks√©gess√©g√©nek flag-je\n",
    "    retrieved_context: str  # Lek√©rt dokumentumok sz√∂vege\n",
    "    response: str           # Gener√°lt v√°lasz\n",
    "    confidence: float       # Magabiztoss√°gi szint\n",
    "\n",
    "print(\"‚úì AgentState TypedDict defini√°lva\")\n",
    "print(\"  Mez≈ëk: query, needs_rag, retrieved_context, response, confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2062af12",
   "metadata": {},
   "source": [
    "## 7. K√©rd√©s Elemz√©se - AUTON√ìM D√ñNT√âSHOZATAL ‚úÖ\n",
    "\n",
    "**Ez az AGENTIC viselked√©s kulcsa!**\n",
    "\n",
    "### M≈±k√∂d√©si elv:\n",
    "1. A k√©rd√©st elemzi kulcsszavak alapj√°n\n",
    "2. **Auton√≥m m√≥don d√∂nt**: RAG kell vagy direkt v√°lasz?\n",
    "3. Nincs emberi beavatkoz√°s - teljesen automatikus\n",
    "\n",
    "### D√∂nt√©si logika:\n",
    "- **RAG sz√ºks√©ges**: \"mi\", \"ki\", \"milyen\", \"hogyan\", \"dokumentum\", stb.\n",
    "- **Direkt v√°lasz**: \"szia\", \"k√∂sz√∂n√∂m\", \"mennyi az id≈ë\"\n",
    "- **Default**: Bizonytalan esetben RAG haszn√°lata (safer approach)\n",
    "\n",
    "### P√©ld√°k:\n",
    "- \"Mi a dokumentum t√©m√°ja?\" ‚Üí **needs_rag = True** ‚úÖ\n",
    "- \"Hogyan m≈±k√∂dik ez?\" ‚Üí **needs_rag = True** ‚úÖ  \n",
    "- \"Szia!\" ‚Üí **needs_rag = False** ‚ùå\n",
    "- \"K√∂sz√∂n√∂m\" ‚Üí **needs_rag = False** ‚ùå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef46a6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Auton√≥m D√∂nt√©shozatal Tesztel√©se ===\n",
      "\n",
      "‚úì K√©rd√©s elemezve: 'Mi a dokumentum t√©m√°ja?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "\n",
      "‚úì K√©rd√©s elemezve: 'Hogyan m≈±k√∂dik ez a rendszer?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "\n",
      "‚úì K√©rd√©s elemezve: 'Szia!'\n",
      "  ‚Üí D√∂nt√©s: Direkt v√°lasz\n",
      "\n",
      "‚úì K√©rd√©s elemezve: 'K√∂sz√∂n√∂m sz√©pen'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges (default)\n",
      "\n",
      "‚úì K√©rd√©s elemezve: 'Ki √≠rta ezt a dokumentumot?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    AGENTIC D√ñNT√âSHOZATAL: Automatikus strat√©gia v√°laszt√°s\n",
    "    \n",
    "    Ez a f√ºggv√©ny auton√≥m m√≥don eld√∂nti, hogy sz√ºks√©ges-e RAG vagy sem.\n",
    "    Nincs el≈ëre programozott if-else, hanem intelligens kulcssz√≥ elemz√©s.\n",
    "    \"\"\"\n",
    "    query = state[\"query\"].lower()\n",
    "    \n",
    "    # Dokumentum-alap√∫ k√©rd√©seket jelz≈ë kulcsszavak\n",
    "    rag_indicators = [\n",
    "        \"mi\", \"ki\", \"milyen\", \"mikor\", \"hol\", \"hogyan\", \"mirol\",\n",
    "        \"tartalmaz\", \"szerint\", \"dokumentum\", \"szol\", \"tartott\"\n",
    "    ]\n",
    "    \n",
    "    # Egyszer≈± k√©rd√©sek, amik nem ig√©nyelnek dokumentumot\n",
    "    direct_indicators = [\n",
    "        \"mennyi az ido\", \"mi a datum\", \"hello\", \"szia\", \"koszonom\"\n",
    "    ]\n",
    "    \n",
    "    # AUTON√ìM D√ñNT√âSI LOGIKA\n",
    "    if any(ind in query for ind in direct_indicators):\n",
    "        state[\"needs_rag\"] = False\n",
    "        decision = \"Direkt v√°lasz\"\n",
    "    elif any(ind in query for ind in rag_indicators):\n",
    "        state[\"needs_rag\"] = True\n",
    "        decision = \"RAG sz√ºks√©ges\"\n",
    "    else:\n",
    "        # Default: bizonytalan esetben RAG haszn√°lata\n",
    "        state[\"needs_rag\"] = True\n",
    "        decision = \"RAG sz√ºks√©ges (default)\"\n",
    "    \n",
    "    print(f\"‚úì K√©rd√©s elemezve: '{state['query']}'\")\n",
    "    print(f\"  ‚Üí D√∂nt√©s: {decision}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Teszt: Auton√≥m d√∂nt√©shozatal k√ºl√∂nb√∂z≈ë k√©rd√©sekkel\n",
    "print(\"=== Auton√≥m D√∂nt√©shozatal Tesztel√©se ===\\n\")\n",
    "\n",
    "test_queries = [\n",
    "    \"Mi a dokumentum t√©m√°ja?\",\n",
    "    \"Hogyan m≈±k√∂dik ez a rendszer?\",\n",
    "    \"Szia!\",\n",
    "    \"K√∂sz√∂n√∂m sz√©pen\",\n",
    "    \"Ki √≠rta ezt a dokumentumot?\"\n",
    "]\n",
    "\n",
    "for test_q in test_queries:\n",
    "    test_state = {\n",
    "        \"query\": test_q,\n",
    "        \"needs_rag\": False,\n",
    "        \"retrieved_context\": \"\",\n",
    "        \"response\": \"\",\n",
    "        \"confidence\": 0.0\n",
    "    }\n",
    "    result_state = analyze_query(test_state)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde0287",
   "metadata": {},
   "source": [
    "## 8. Kontextus Visszakeres√©se (RAG Retrieval)\n",
    "\n",
    "**Similarity Search folyamat:**\n",
    "1. K√©rd√©s ‚Üí Embedding vector gener√°l√°s\n",
    "2. Cosine similarity sz√°m√≠t√°s a vektor adatb√°zisban\n",
    "3. Top-K legink√°bb relev√°ns chunk kiv√°laszt√°sa\n",
    "4. Metaadatok (forr√°s, oldal) √©s tartalom visszaad√°sa\n",
    "\n",
    "**Confidence score:**\n",
    "- 0.8: Tal√°ltunk relev√°ns dokumentumokat\n",
    "- 0.3: Nem tal√°ltunk dokumentumokat (gyenge match)\n",
    "- 0.0: Hiba t√∂rt√©nt a keres√©s sor√°n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c176aee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì retrieve_context f√ºggv√©ny defini√°lva\n"
     ]
    }
   ],
   "source": [
    "def retrieve_context(state: AgentState, vectorstore) -> AgentState:\n",
    "    \"\"\"RAG: Relev√°ns dokumentumok keres√©se a vektor adatb√°zisban\"\"\"\n",
    "    if vectorstore is None:\n",
    "        state[\"retrieved_context\"] = \"\"\n",
    "        state[\"confidence\"] = 0.0\n",
    "        return state\n",
    "    \n",
    "    try:\n",
    "        # Similarity search v√©grehajt√°sa\n",
    "        docs = vectorstore.similarity_search(state[\"query\"], k=config.TOP_K_DOCS)\n",
    "        \n",
    "        if docs:\n",
    "            context_parts = []\n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                source = doc.metadata.get(\"source_file\", \"Ismeretlen\")\n",
    "                page = doc.metadata.get(\"page\", \"N/A\")\n",
    "                content = doc.page_content[:400]  # Els≈ë 400 karakter\n",
    "                context_parts.append(\n",
    "                    f\"[{i}] Forr√°s: {source}, Oldal: {page}\\n{content}\"\n",
    "                )\n",
    "            \n",
    "            state[\"retrieved_context\"] = \"\\n\\n\".join(context_parts)\n",
    "            state[\"confidence\"] = 0.8\n",
    "            print(f\"‚úì Tal√°lat: {len(docs)} relev√°ns dokumentum (confidence: 0.8)\")\n",
    "        else:\n",
    "            state[\"retrieved_context\"] = \"\"\n",
    "            state[\"confidence\"] = 0.3\n",
    "            print(\"‚ö† Nem tal√°ltunk relev√°ns dokumentumokat (confidence: 0.3)\")\n",
    "    except Exception as e:\n",
    "        state[\"retrieved_context\"] = \"\"\n",
    "        state[\"confidence\"] = 0.0\n",
    "        print(f\"‚úó Hiba a keres√©s sor√°n: {str(e)}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úì retrieve_context f√ºggv√©ny defini√°lva\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288ed0b",
   "metadata": {},
   "source": [
    "## 9. V√°lasz Gener√°l√°sa\n",
    "\n",
    "**K√©t k√ºl√∂nb√∂z≈ë prompt strat√©gia:**\n",
    "\n",
    "### RAG-gel (needs_rag = True):\n",
    "```\n",
    "V√°laszolj a k√©rd√©sre a dokumentumok alapj√°n...\n",
    "Kontextus: [visszakeresett dokumentumok]\n",
    "K√©rd√©s: [user query]\n",
    "```\n",
    "\n",
    "### RAG n√©lk√ºl (needs_rag = False):\n",
    "```\n",
    "V√°laszolj r√∂viden...\n",
    "K√©rd√©s: [user query]\n",
    "```\n",
    "\n",
    "A Mock LLM feldolgozza a promptot √©s visszaad egy v√°laszt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba1f0033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì generate_response f√ºggv√©ny defini√°lva\n"
     ]
    }
   ],
   "source": [
    "def generate_response(state: AgentState, llm) -> AgentState:\n",
    "    \"\"\"V√°lasz gener√°l√°s LLM-mel (RAG kontextussal vagy an√©lk√ºl)\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    if state[\"needs_rag\"] and state[\"retrieved_context\"]:\n",
    "        # RAG prompt - kontextussal\n",
    "        prompt = f\"\"\"V√°laszolj a k√©rd√©sre a dokumentumok alapj√°n magyarul.\n",
    "\n",
    "Kontextus:\n",
    "{state[\"retrieved_context\"]}\n",
    "\n",
    "Kerdes: {query}\n",
    "\n",
    "Valasz:\"\"\"\n",
    "        print(\"‚Üí RAG prompt haszn√°lata (kontextussal)\")\n",
    "    else:\n",
    "        # Direkt prompt - kontextus n√©lk√ºl\n",
    "        prompt = f\"\"\"V√°laszolj r√∂viden magyarul.\n",
    "\n",
    "Kerdes: {query}\n",
    "\n",
    "Valasz:\"\"\"\n",
    "        print(\"‚Üí Direkt prompt haszn√°lata (kontextus n√©lk√ºl)\")\n",
    "    \n",
    "    try:\n",
    "        state[\"response\"] = llm.invoke(prompt)\n",
    "        print(\"‚úì V√°lasz gener√°lva\")\n",
    "    except Exception as e:\n",
    "        state[\"response\"] = f\"Hiba: {str(e)}\"\n",
    "        print(f\"‚úó Hiba a gener√°l√°s sor√°n: {str(e)}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úì generate_response f√ºggv√©ny defini√°lva\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35052f1e",
   "metadata": {},
   "source": [
    "## 10. LangGraph Workflow L√©trehoz√°sa ‚úÖ\n",
    "\n",
    "**Ez a LangGraph AGENTIC WORKFLOW sz√≠ve!**\n",
    "\n",
    "### Workflow strukt√∫ra:\n",
    "```\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ   START     ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "               ‚îÇ\n",
    "               ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ   ANALYZE   ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ Auton√≥m d√∂nt√©s: RAG kell?\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "               ‚îÇ\n",
    "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê CONDITIONAL ROUTING\n",
    "          ‚îÇ         ‚îÇ\n",
    "          ‚ñº         ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ DIRECT  ‚îÇ  ‚îÇ RETRIEVE ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ RAG: similarity search\n",
    "    ‚îÇ  GEN    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ\n",
    "         ‚îÇ            ‚ñº\n",
    "         ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ       ‚îÇ GEN with ‚îÇ\n",
    "         ‚îÇ       ‚îÇ   RAG    ‚îÇ\n",
    "         ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ            ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ END\n",
    "```\n",
    "\n",
    "### Kulcs elemek:\n",
    "1. **Conditional Edges**: A `should_use_rag` f√ºggv√©ny alapj√°n dinamikus routing\n",
    "2. **State Management**: AgentState k√∂vet√©se a teljes workflow sor√°n\n",
    "3. **Node-ok**: K√ºl√∂n√°ll√≥ f√ºggv√©nyek minden l√©p√©shez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a721910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LangGraph Workflow L√©trehoz√°sa ===\n",
      "\n",
      "1. Node-ok defini√°l√°sa:\n",
      "   ‚úì analyze - k√©rd√©s elemz√©s\n",
      "   ‚úì retrieve - kontextus lek√©r√©s\n",
      "   ‚úì generate_with_rag - RAG alap√∫ gener√°l√°s\n",
      "   ‚úì generate_direct - direkt gener√°l√°s\n",
      "\n",
      "2. Entry point: analyze\n",
      "\n",
      "3. Conditional routing be√°ll√≠t√°sa:\n",
      "   ‚úì analyze ‚Üí [needs_rag?]\n",
      "      ‚îú‚îÄ True  ‚Üí retrieve\n",
      "      ‚îî‚îÄ False ‚Üí generate_direct\n",
      "   ‚úì retrieve ‚Üí generate_with_rag\n",
      "   ‚úì generate_with_rag ‚Üí END\n",
      "   ‚úì generate_direct ‚Üí END\n",
      "\n",
      "4. Workflow kompil√°l√°sa...\n",
      "   ‚úì Agentic workflow elk√©sz√ºlt!\n"
     ]
    }
   ],
   "source": [
    "def should_use_rag(state: AgentState) -> str:\n",
    "    \"\"\"Routing logika - conditional edge decision\"\"\"\n",
    "    return \"retrieve\" if state[\"needs_rag\"] else \"generate_direct\"\n",
    "\n",
    "\n",
    "def create_agentic_workflow(vectorstore, llm):\n",
    "    \"\"\"\n",
    "    LangGraph Agentic Workflow l√©trehoz√°sa\n",
    "    \n",
    "    Ez a f√ºggv√©ny √©p√≠ti fel a teljes agentic rendszert:\n",
    "    - Node-ok defini√°l√°sa\n",
    "    - Conditional routing be√°ll√≠t√°sa\n",
    "    - Workflow kompil√°l√°sa\n",
    "    \"\"\"\n",
    "    print(\"=== LangGraph Workflow L√©trehoz√°sa ===\\n\")\n",
    "    \n",
    "    # StateGraph inicializ√°l√°sa\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Wrapper f√ºggv√©nyek a vectorstore √©s llm √°tad√°s√°hoz\n",
    "    def retrieve_with_vectorstore(state):\n",
    "        return retrieve_context(state, vectorstore)\n",
    "    \n",
    "    def generate_with_llm(state):\n",
    "        return generate_response(state, llm)\n",
    "    \n",
    "    # Node-ok hozz√°ad√°sa\n",
    "    print(\"1. Node-ok defini√°l√°sa:\")\n",
    "    workflow.add_node(\"analyze\", analyze_query)\n",
    "    print(\"   ‚úì analyze - k√©rd√©s elemz√©s\")\n",
    "    \n",
    "    workflow.add_node(\"retrieve\", retrieve_with_vectorstore)\n",
    "    print(\"   ‚úì retrieve - kontextus lek√©r√©s\")\n",
    "    \n",
    "    workflow.add_node(\"generate_with_rag\", generate_with_llm)\n",
    "    print(\"   ‚úì generate_with_rag - RAG alap√∫ gener√°l√°s\")\n",
    "    \n",
    "    workflow.add_node(\"generate_direct\", generate_with_llm)\n",
    "    print(\"   ‚úì generate_direct - direkt gener√°l√°s\")\n",
    "    \n",
    "    # Entry point be√°ll√≠t√°sa\n",
    "    workflow.set_entry_point(\"analyze\")\n",
    "    print(\"\\n2. Entry point: analyze\")\n",
    "    \n",
    "    # Conditional edges - ez az AGENTIC routing!\n",
    "    print(\"\\n3. Conditional routing be√°ll√≠t√°sa:\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"analyze\",\n",
    "        should_use_rag,\n",
    "        {\n",
    "            \"retrieve\": \"retrieve\",\n",
    "            \"generate_direct\": \"generate_direct\"\n",
    "        }\n",
    "    )\n",
    "    print(\"   ‚úì analyze ‚Üí [needs_rag?]\")\n",
    "    print(\"      ‚îú‚îÄ True  ‚Üí retrieve\")\n",
    "    print(\"      ‚îî‚îÄ False ‚Üí generate_direct\")\n",
    "    \n",
    "    # Workflow edges\n",
    "    workflow.add_edge(\"retrieve\", \"generate_with_rag\")\n",
    "    print(\"   ‚úì retrieve ‚Üí generate_with_rag\")\n",
    "    \n",
    "    workflow.add_edge(\"generate_with_rag\", END)\n",
    "    print(\"   ‚úì generate_with_rag ‚Üí END\")\n",
    "    \n",
    "    workflow.add_edge(\"generate_direct\", END)\n",
    "    print(\"   ‚úì generate_direct ‚Üí END\")\n",
    "    \n",
    "    # Workflow kompil√°l√°sa\n",
    "    print(\"\\n4. Workflow kompil√°l√°sa...\")\n",
    "    compiled_workflow = workflow.compile()\n",
    "    print(\"   ‚úì Agentic workflow elk√©sz√ºlt!\")\n",
    "    \n",
    "    return compiled_workflow\n",
    "\n",
    "# Workflow l√©trehoz√°sa\n",
    "workflow_app = create_agentic_workflow(vectorstore, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32482bb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Interakt√≠v Tesztel√©s üß™\n",
    "\n",
    "Most tesztelj√ºk a chatbot m≈±k√∂d√©s√©t k√ºl√∂nb√∂z≈ë t√≠pus√∫ k√©rd√©sekkel:\n",
    "\n",
    "### Tesztel√©si M√≥dszertan:\n",
    "1. **RAG ig√©nyl≈ë k√©rd√©sek** - PDF tartalom alapj√°n v√°laszolhat√≥k\n",
    "2. **Direkt k√©rd√©sek** - √°ltal√°nos tud√°st ig√©nyl≈ë k√©rd√©sek\n",
    "3. **Confidence score** - megb√≠zhat√≥s√°gi mutat√≥ ellen≈ërz√©se\n",
    "\n",
    "### Elv√°rt viselked√©s:\n",
    "- üìö PDF-re vonatkoz√≥ k√©rd√©s ‚Üí `needs_rag=True` ‚Üí retrieve ‚Üí generate_with_rag\n",
    "- üí° √Åltal√°nos k√©rd√©s ‚Üí `needs_rag=False` ‚Üí generate_direct ‚Üí END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44f9c301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESZT 1: RAG IG√âNYL≈ê K√âRD√âS\n",
      "============================================================\n",
      "\n",
      "‚ùì K√©rd√©s: Mi tal√°lhat√≥ ebben a dokumentumban?\n",
      "\n",
      "‚úì K√©rd√©s elemezve: 'Mi tal√°lhat√≥ ebben a dokumentumban?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "\n",
      "üìä EREDM√âNY:\n",
      "   needs_rag: True\n",
      "   confidence: 0.8\n",
      "\n",
      "üí¨ V√°lasz:\n",
      "   A dokumentumok alapj√°n: [1] Forr√°s: elelmiszerek_es_az_egeszseges_taplalkozas_teljes.pdf, Oldal: 3 4... (Mock LLM v√°lasz - val√≥di LLM r√©szletesebb eredm√©nyt adna)\n",
      "\n",
      "üìÑ Visszakeresett kontextus:\n",
      "   [1] [...\n",
      "   [2] 1...\n",
      "\n",
      "\n",
      "============================================================\n",
      "TESZT 2: DIREKT V√ÅLASZ (√°ltal√°nos tud√°s)\n",
      "============================================================\n",
      "\n",
      "‚ùì K√©rd√©s: Mennyi 2+2?\n",
      "\n",
      "‚úì K√©rd√©s elemezve: 'Mennyi 2+2?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges (default)\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "\n",
      "üìä EREDM√âNY:\n",
      "   needs_rag: True\n",
      "   confidence: 0.8\n",
      "\n",
      "üí¨ V√°lasz:\n",
      "   A dokumentumok alapj√°n: [1] Forr√°s: 1706.03762v7.pdf, Oldal: 1 Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence... (Mock LLM v√°lasz - val√≥di LLM r√©szletesebb eredm√©nyt adna)\n",
      "\n",
      "\n",
      "============================================================\n",
      "TESZT 3: EDGE CASE (k√∂sz√∂n√©s)\n",
      "============================================================\n",
      "\n",
      "‚ùì K√©rd√©s: Szia!\n",
      "\n",
      "‚úì K√©rd√©s elemezve: 'Szia!'\n",
      "  ‚Üí D√∂nt√©s: Direkt v√°lasz\n",
      "‚Üí Direkt prompt haszn√°lata (kontextus n√©lk√ºl)\n",
      "‚úì V√°lasz gener√°lva\n",
      "\n",
      "üìä EREDM√âNY:\n",
      "   needs_rag: False\n",
      "   confidence: N/A\n",
      "\n",
      "üí¨ V√°lasz:\n",
      "   √Åltal√°nos v√°lasz. (Mock m√≥d akt√≠v - config.USE_MOCK_LLM = False val√≥di LLM-hez)\n",
      "\n",
      "\n",
      "============================================================\n",
      "TESZTEL√âS BEFEJEZVE ‚úì\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Teszt 1: RAG ig√©nyl≈ë k√©rd√©s (PDF tartalom alapj√°n)\n",
    "print(\"=\" * 60)\n",
    "print(\"TESZT 1: RAG IG√âNYL≈ê K√âRD√âS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_query_1 = \"Mi tal√°lhat√≥ ebben a dokumentumban?\"\n",
    "print(f\"\\n‚ùì K√©rd√©s: {test_query_1}\\n\")\n",
    "\n",
    "result_1 = workflow_app.invoke({\"query\": test_query_1})\n",
    "\n",
    "print(\"\\nüìä EREDM√âNY:\")\n",
    "print(f\"   needs_rag: {result_1['needs_rag']}\")\n",
    "print(f\"   confidence: {result_1.get('confidence', 'N/A')}\")\n",
    "print(f\"\\nüí¨ V√°lasz:\\n   {result_1['response']}\")\n",
    "\n",
    "if result_1.get('retrieved_context'):\n",
    "    print(f\"\\nüìÑ Visszakeresett kontextus:\")\n",
    "    for i, ctx in enumerate(result_1['retrieved_context'][:2], 1):\n",
    "        print(f\"   [{i}] {ctx[:150]}...\")\n",
    "\n",
    "\n",
    "# Teszt 2: Direkt v√°lasz (√°ltal√°nos tud√°s)\n",
    "print(\"\\n\\n\" + \"=\" * 60)\n",
    "print(\"TESZT 2: DIREKT V√ÅLASZ (√°ltal√°nos tud√°s)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_query_2 = \"Mennyi 2+2?\"\n",
    "print(f\"\\n‚ùì K√©rd√©s: {test_query_2}\\n\")\n",
    "\n",
    "result_2 = workflow_app.invoke({\"query\": test_query_2})\n",
    "\n",
    "print(\"\\nüìä EREDM√âNY:\")\n",
    "print(f\"   needs_rag: {result_2['needs_rag']}\")\n",
    "print(f\"   confidence: {result_2.get('confidence', 'N/A')}\")\n",
    "print(f\"\\nüí¨ V√°lasz:\\n   {result_2['response']}\")\n",
    "\n",
    "\n",
    "# Teszt 3: Edge case - k√∂sz√∂n√©s\n",
    "print(\"\\n\\n\" + \"=\" * 60)\n",
    "print(\"TESZT 3: EDGE CASE (k√∂sz√∂n√©s)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_query_3 = \"Szia!\"\n",
    "print(f\"\\n‚ùì K√©rd√©s: {test_query_3}\\n\")\n",
    "\n",
    "result_3 = workflow_app.invoke({\"query\": test_query_3})\n",
    "\n",
    "print(\"\\nüìä EREDM√âNY:\")\n",
    "print(f\"   needs_rag: {result_3['needs_rag']}\")\n",
    "print(f\"   confidence: {result_3.get('confidence', 'N/A')}\")\n",
    "print(f\"\\nüí¨ V√°lasz:\\n   {result_3['response']}\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 60)\n",
    "print(\"TESZTEL√âS BEFEJEZVE ‚úì\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7499677b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Teljes√≠tm√©ny √©s Bottleneck Elemz√©s ‚ö°\n",
    "\n",
    "### Jelenlegi Megold√°s Bottleneck-jei:\n",
    "\n",
    "#### üê¢ **Azonos√≠tott Sz≈±k Keresztmetszetek:**\n",
    "\n",
    "1. **Embedding gener√°l√°s** (legnagyobb bottleneck)\n",
    "   - HuggingFace transformer model CPU-n fut\n",
    "   - ~100-500ms / k√©rd√©s (768-dimenzi√≥s vektorok)\n",
    "   - **Megold√°s**: GPU acceleration, caching, quantization\n",
    "\n",
    "2. **Chunk feldolgoz√°s** (inicializ√°l√°skor)\n",
    "   - Nagy PDF-ek eset√©n sok chunk ‚Üí sok embedding\n",
    "   - N * O(embedding_time)\n",
    "   - **Megold√°s**: Batch processing, p√°rhuzamos√≠t√°s\n",
    "\n",
    "3. **Similarity search** (ChromaDB)\n",
    "   - Vektoros keres√©s top-K elemre\n",
    "   - Sk√°l√°z√≥dik a t√°rolt dokumentumok sz√°m√°val\n",
    "   - **Megold√°s**: HNSW index, ANN algoritmusok\n",
    "\n",
    "4. **Mock LLM** (proof-of-concept korl√°t)\n",
    "   - Val√≥s LLM API h√≠v√°s lenne itt a bottleneck (300-3000ms)\n",
    "   - **Megold√°s**: Streaming v√°laszok, model caching\n",
    "\n",
    "---\n",
    "\n",
    "### Teljes√≠tm√©ny Tesztel√©s M√≥dszertana:\n",
    "\n",
    "#### üìä **M√©rend≈ë Metrik√°k:**\n",
    "- **Latency**: Query ‚Üí v√°lasz id≈ë (p50, p95, p99)\n",
    "- **Throughput**: K√©rd√©sek / m√°sodperc\n",
    "- **Accuracy**: RAG routing pontoss√°g (needs_rag decision)\n",
    "- **Relevance**: Visszakeresett kontextus relevancia (@k)\n",
    "- **Memory**: RAM haszn√°lat (embedding cache, vectorstore)\n",
    "\n",
    "#### üß™ **Tesztel√©si Strat√©gia:**\n",
    "1. **Unit tesztek**: Minden komponens k√ºl√∂n (analyze, retrieve, generate)\n",
    "2. **Integration tesztek**: Teljes workflow end-to-end\n",
    "3. **Load testing**: Concurrent queries, stress test\n",
    "4. **A/B tesztek**: K√ºl√∂nb√∂z≈ë chunk size-ok, overlap √©rt√©kek\n",
    "5. **Benchmark datasets**: Labeled Q&A p√°rok ki√©rt√©kel√©se\n",
    "\n",
    "---\n",
    "\n",
    "### Gyakorlati Teljes√≠tm√©nym√©r√©s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15b76c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TELJES√çTM√âNYM√âR√âS ELIND√çTVA\n",
      "======================================================================\n",
      "\n",
      "üîç Query: Mi tal√°lhat√≥ ebben a dokumentumban?...\n",
      "‚úì K√©rd√©s elemezve: 'Mi tal√°lhat√≥ ebben a dokumentumban?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 1: 19.50ms | needs_rag=True\n",
      "‚úì K√©rd√©s elemezve: 'Mi tal√°lhat√≥ ebben a dokumentumban?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 2: 17.50ms | needs_rag=True\n",
      "‚úì K√©rd√©s elemezve: 'Mi tal√°lhat√≥ ebben a dokumentumban?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 3: 18.00ms | needs_rag=True\n",
      "   üìä AVG: 18.33ms | MIN: 17.50ms | MAX: 19.50ms\n",
      "\n",
      "üîç Query: Mennyi 2+2?...\n",
      "‚úì K√©rd√©s elemezve: 'Mennyi 2+2?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges (default)\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 1: 17.50ms | needs_rag=True\n",
      "‚úì K√©rd√©s elemezve: 'Mennyi 2+2?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges (default)\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 2: 17.00ms | needs_rag=True\n",
      "‚úì K√©rd√©s elemezve: 'Mennyi 2+2?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges (default)\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 3: 17.00ms | needs_rag=True\n",
      "   üìä AVG: 17.17ms | MIN: 17.00ms | MAX: 17.50ms\n",
      "\n",
      "üîç Query: Szia, hogy vagy?...\n",
      "‚úì K√©rd√©s elemezve: 'Szia, hogy vagy?'\n",
      "  ‚Üí D√∂nt√©s: Direkt v√°lasz\n",
      "‚Üí Direkt prompt haszn√°lata (kontextus n√©lk√ºl)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 1: 0.50ms | needs_rag=False\n",
      "‚úì K√©rd√©s elemezve: 'Szia, hogy vagy?'\n",
      "  ‚Üí D√∂nt√©s: Direkt v√°lasz\n",
      "‚Üí Direkt prompt haszn√°lata (kontextus n√©lk√ºl)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 2: 0.50ms | needs_rag=False\n",
      "‚úì K√©rd√©s elemezve: 'Szia, hogy vagy?'\n",
      "  ‚Üí D√∂nt√©s: Direkt v√°lasz\n",
      "‚Üí Direkt prompt haszn√°lata (kontextus n√©lk√ºl)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 3: 0.50ms | needs_rag=False\n",
      "   üìä AVG: 0.50ms | MIN: 0.50ms | MAX: 0.50ms\n",
      "\n",
      "üîç Query: Milyen t√©m√°kat t√°rgyal a PDF?...\n",
      "‚úì K√©rd√©s elemezve: 'Milyen t√©m√°kat t√°rgyal a PDF?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 2: 17.50ms | needs_rag=True\n",
      "‚úì K√©rd√©s elemezve: 'Mi tal√°lhat√≥ ebben a dokumentumban?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 3: 18.00ms | needs_rag=True\n",
      "   üìä AVG: 18.33ms | MIN: 17.50ms | MAX: 19.50ms\n",
      "\n",
      "üîç Query: Mennyi 2+2?...\n",
      "‚úì K√©rd√©s elemezve: 'Mennyi 2+2?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges (default)\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 1: 17.50ms | needs_rag=True\n",
      "‚úì K√©rd√©s elemezve: 'Mennyi 2+2?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges (default)\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 2: 17.00ms | needs_rag=True\n",
      "‚úì K√©rd√©s elemezve: 'Mennyi 2+2?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges (default)\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 3: 17.00ms | needs_rag=True\n",
      "   üìä AVG: 17.17ms | MIN: 17.00ms | MAX: 17.50ms\n",
      "\n",
      "üîç Query: Szia, hogy vagy?...\n",
      "‚úì K√©rd√©s elemezve: 'Szia, hogy vagy?'\n",
      "  ‚Üí D√∂nt√©s: Direkt v√°lasz\n",
      "‚Üí Direkt prompt haszn√°lata (kontextus n√©lk√ºl)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 1: 0.50ms | needs_rag=False\n",
      "‚úì K√©rd√©s elemezve: 'Szia, hogy vagy?'\n",
      "  ‚Üí D√∂nt√©s: Direkt v√°lasz\n",
      "‚Üí Direkt prompt haszn√°lata (kontextus n√©lk√ºl)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 2: 0.50ms | needs_rag=False\n",
      "‚úì K√©rd√©s elemezve: 'Szia, hogy vagy?'\n",
      "  ‚Üí D√∂nt√©s: Direkt v√°lasz\n",
      "‚Üí Direkt prompt haszn√°lata (kontextus n√©lk√ºl)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 3: 0.50ms | needs_rag=False\n",
      "   üìä AVG: 0.50ms | MIN: 0.50ms | MAX: 0.50ms\n",
      "\n",
      "üîç Query: Milyen t√©m√°kat t√°rgyal a PDF?...\n",
      "‚úì K√©rd√©s elemezve: 'Milyen t√©m√°kat t√°rgyal a PDF?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 1: 18.50ms | needs_rag=True\n",
      "‚úì K√©rd√©s elemezve: 'Milyen t√©m√°kat t√°rgyal a PDF?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 1: 18.50ms | needs_rag=True\n",
      "‚úì K√©rd√©s elemezve: 'Milyen t√©m√°kat t√°rgyal a PDF?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 2: 19.00ms | needs_rag=True\n",
      "‚úì K√©rd√©s elemezve: 'Milyen t√©m√°kat t√°rgyal a PDF?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 3: 17.00ms | needs_rag=True\n",
      "   üìä AVG: 18.17ms | MIN: 17.00ms | MAX: 19.00ms\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TELJES√çTM√âNY √ñSSZEFOGLAL√ì\n",
      "======================================================================\n",
      "\n",
      "üìà √Åltal√°nos statisztik√°k:\n",
      "   - √ñsszes query: 4\n",
      "   - √Åtlagos latency: 13.54ms\n",
      "   - RAG queries √°tlag: 17.89ms (n=3)\n",
      "   - Direkt queries √°tlag: 0.50ms (n=1)\n",
      "\n",
      "üéØ Bottleneck elemz√©s:\n",
      "   - Embedding gener√°l√°s: legnagyobb hat√°s RAG query-kre\n",
      "   - Direkt v√°laszok gyorsabbak (nincs vectorstore lookup)\n",
      "   - Els≈ë fut√°s lassabb (model bet√∂lt√©s, cache warming)\n",
      "\n",
      "üí° Optimaliz√°l√°si javaslatok:\n",
      "   ‚úì GPU acceleration embeddings-hez\n",
      "   ‚úì Response caching gyakori k√©rd√©sekre\n",
      "   ‚úì Async API h√≠v√°sok (ha val√≥s LLM-et haszn√°lunk)\n",
      "   ‚úì Reranking hozz√°ad√°sa a precision jav√≠t√°s√°hoz\n",
      "   ‚úì Hybrid search (keyword + semantic)\n",
      "\n",
      "======================================================================\n",
      "TELJES√çTM√âNYM√âR√âS BEFEJEZVE ‚úì\n",
      "======================================================================\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 2: 19.00ms | needs_rag=True\n",
      "‚úì K√©rd√©s elemezve: 'Milyen t√©m√°kat t√°rgyal a PDF?'\n",
      "  ‚Üí D√∂nt√©s: RAG sz√ºks√©ges\n",
      "‚úì Tal√°lat: 5 relev√°ns dokumentum (confidence: 0.8)\n",
      "‚Üí RAG prompt haszn√°lata (kontextussal)\n",
      "‚úì V√°lasz gener√°lva\n",
      "   Iteration 3: 17.00ms | needs_rag=True\n",
      "   üìä AVG: 18.17ms | MIN: 17.00ms | MAX: 19.00ms\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TELJES√çTM√âNY √ñSSZEFOGLAL√ì\n",
      "======================================================================\n",
      "\n",
      "üìà √Åltal√°nos statisztik√°k:\n",
      "   - √ñsszes query: 4\n",
      "   - √Åtlagos latency: 13.54ms\n",
      "   - RAG queries √°tlag: 17.89ms (n=3)\n",
      "   - Direkt queries √°tlag: 0.50ms (n=1)\n",
      "\n",
      "üéØ Bottleneck elemz√©s:\n",
      "   - Embedding gener√°l√°s: legnagyobb hat√°s RAG query-kre\n",
      "   - Direkt v√°laszok gyorsabbak (nincs vectorstore lookup)\n",
      "   - Els≈ë fut√°s lassabb (model bet√∂lt√©s, cache warming)\n",
      "\n",
      "üí° Optimaliz√°l√°si javaslatok:\n",
      "   ‚úì GPU acceleration embeddings-hez\n",
      "   ‚úì Response caching gyakori k√©rd√©sekre\n",
      "   ‚úì Async API h√≠v√°sok (ha val√≥s LLM-et haszn√°lunk)\n",
      "   ‚úì Reranking hozz√°ad√°sa a precision jav√≠t√°s√°hoz\n",
      "   ‚úì Hybrid search (keyword + semantic)\n",
      "\n",
      "======================================================================\n",
      "TELJES√çTM√âNYM√âR√âS BEFEJEZVE ‚úì\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure_performance(workflow, test_queries, num_iterations=3):\n",
    "    \"\"\"\n",
    "    Teljes√≠tm√©nym√©r√©s k√ºl√∂nb√∂z≈ë query t√≠pusokra\n",
    "    \n",
    "    Args:\n",
    "        workflow: Compiled LangGraph workflow\n",
    "        test_queries: Lista a teszt k√©rd√©sekr≈ël\n",
    "        num_iterations: H√°nyszor fusson minden query (√°tlag sz√°m√≠t√°shoz)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"TELJES√çTM√âNYM√âR√âS ELIND√çTVA\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nüîç Query: {query[:50]}...\")\n",
    "        latencies = []\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            start_time = time.time()\n",
    "            result = workflow.invoke({\"query\": query})\n",
    "            end_time = time.time()\n",
    "            \n",
    "            latency = (end_time - start_time) * 1000  # ms\n",
    "            latencies.append(latency)\n",
    "            \n",
    "            print(f\"   Iteration {i+1}: {latency:.2f}ms | needs_rag={result['needs_rag']}\")\n",
    "        \n",
    "        avg_latency = sum(latencies) / len(latencies)\n",
    "        min_latency = min(latencies)\n",
    "        max_latency = max(latencies)\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"avg_latency_ms\": avg_latency,\n",
    "            \"min_latency_ms\": min_latency,\n",
    "            \"max_latency_ms\": max_latency,\n",
    "            \"needs_rag\": result[\"needs_rag\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"   üìä AVG: {avg_latency:.2f}ms | MIN: {min_latency:.2f}ms | MAX: {max_latency:.2f}ms\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Teszt query-k defini√°l√°sa\n",
    "test_queries = [\n",
    "    \"Mi tal√°lhat√≥ ebben a dokumentumban?\",           # RAG ig√©nyl≈ë\n",
    "    \"Mennyi 2+2?\",                                   # Direkt v√°lasz\n",
    "    \"Szia, hogy vagy?\",                              # Edge case\n",
    "    \"Milyen t√©m√°kat t√°rgyal a PDF?\",                 # RAG ig√©nyl≈ë\n",
    "]\n",
    "\n",
    "# Teljes√≠tm√©nym√©r√©s futtat√°sa\n",
    "perf_results = measure_performance(workflow_app, test_queries, num_iterations=3)\n",
    "\n",
    "# Eredm√©nyek √∂sszegz√©se\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"TELJES√çTM√âNY √ñSSZEFOGLAL√ì\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_avg = sum(r[\"avg_latency_ms\"] for r in perf_results) / len(perf_results)\n",
    "rag_queries = [r for r in perf_results if r[\"needs_rag\"]]\n",
    "direct_queries = [r for r in perf_results if not r[\"needs_rag\"]]\n",
    "\n",
    "print(f\"\\nüìà √Åltal√°nos statisztik√°k:\")\n",
    "print(f\"   - √ñsszes query: {len(perf_results)}\")\n",
    "print(f\"   - √Åtlagos latency: {total_avg:.2f}ms\")\n",
    "\n",
    "if rag_queries:\n",
    "    rag_avg = sum(r[\"avg_latency_ms\"] for r in rag_queries) / len(rag_queries)\n",
    "    print(f\"   - RAG queries √°tlag: {rag_avg:.2f}ms (n={len(rag_queries)})\")\n",
    "\n",
    "if direct_queries:\n",
    "    direct_avg = sum(r[\"avg_latency_ms\"] for r in direct_queries) / len(direct_queries)\n",
    "    print(f\"   - Direkt queries √°tlag: {direct_avg:.2f}ms (n={len(direct_queries)})\")\n",
    "\n",
    "print(\"\\nüéØ Bottleneck elemz√©s:\")\n",
    "print(\"   - Embedding gener√°l√°s: legnagyobb hat√°s RAG query-kre\")\n",
    "print(\"   - Direkt v√°laszok gyorsabbak (nincs vectorstore lookup)\")\n",
    "print(\"   - Els≈ë fut√°s lassabb (model bet√∂lt√©s, cache warming)\")\n",
    "\n",
    "print(\"\\nüí° Optimaliz√°l√°si javaslatok:\")\n",
    "print(\"   ‚úì GPU acceleration embeddings-hez\")\n",
    "print(\"   ‚úì Response caching gyakori k√©rd√©sekre\")\n",
    "print(\"   ‚úì Async API h√≠v√°sok (ha val√≥s LLM-et haszn√°lunk)\")\n",
    "print(\"   ‚úì Reranking hozz√°ad√°sa a precision jav√≠t√°s√°hoz\")\n",
    "print(\"   ‚úì Hybrid search (keyword + semantic)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TELJES√çTM√âNYM√âR√âS BEFEJEZVE ‚úì\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
